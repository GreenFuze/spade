#!/usr/bin/env python3
"""
Test V7 Phase 1 with explore_repository_signals tool on MetaFFI repository.

This test verifies that Phase 1 uses the new comprehensive tool
on a large, complex multi-language repository and provides deep analysis.
"""

import asyncio
import json
import logging
import sys
import time
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from llm0.v7.phase1_repository_overview_agent_v7 import RepositoryOverviewAgentV7


async def test_phase1_with_new_tool():
    """Test Phase 1 with the new explore_repository_signals tool and provide deep analysis."""
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Test repository - MetaFFI
    test_repo = Path("C:/src/github.com/MetaFFI")
    
    if not test_repo.exists():
        print(f"âŒ MetaFFI repository not found: {test_repo}")
        return False
    
    print(f"ğŸ§ª Testing V7 Phase 1 with explore_repository_signals tool on MetaFFI")
    print(f"ğŸ“ Repository: {test_repo}")
    print("=" * 80)
    
    # Analysis tracking
    analysis_data = {
        "start_time": time.time(),
        "tool_usage": [],
        "token_usage": {"input": 0, "output": 0, "total": 0},
        "timing": {},
        "llm_calls": 0,
        "tool_calls": 0,
        "errors": []
    }
    
    try:
        # Create Phase 1 agent
        print("ğŸ”§ Initializing Phase 1 agent...")
        agent_init_start = time.time()
        agent = RepositoryOverviewAgentV7(test_repo)
        agent_init_time = time.time() - agent_init_start
        
        analysis_data["timing"]["agent_initialization"] = agent_init_time
        print(f"   â±ï¸  Agent initialization: {agent_init_time:.2f}s")
        
        # Check if Phase1Tools is properly initialized
        if not hasattr(agent, 'phase1_tools'):
            print("âŒ Phase1Tools not initialized!")
            return False
        
        if not hasattr(agent.phase1_tools, 'explore_repository_signals'):
            print("âŒ explore_repository_signals method not found!")
            return False
        
        print("âœ… Phase1Tools properly initialized with explore_repository_signals method")
        
        # Execute Phase 1 with detailed timing
        print("\nğŸš€ Executing Phase 1...")
        execution_start = time.time()
        
        # Monitor the agent's conversation history for analysis
        original_history = agent.agent._history.copy() if hasattr(agent.agent, '_history') else []
        
        result = await agent.execute_phase()
        
        execution_time = time.time() - execution_start
        analysis_data["timing"]["execution"] = execution_time
        analysis_data["timing"]["total"] = time.time() - analysis_data["start_time"]
        
        print(f"   â±ï¸  Execution time: {execution_time:.2f}s")
        print(f"   â±ï¸  Total time: {analysis_data['timing']['total']:.2f}s")
        
        # Analyze conversation history
        if hasattr(agent.agent, '_history'):
            history = agent.agent._history
            print(f"   ğŸ” History type: {type(history)}")
            print(f"   ğŸ” History length: {len(history) if hasattr(history, '__len__') else 'N/A'}")
            
            # Count LLM calls (user messages)
            if isinstance(history, list):
                analysis_data["llm_calls"] = len([msg for msg in history if isinstance(msg, dict) and msg.get("role") == "user"])
                
                # Count tool calls and analyze usage
                tool_calls = [msg for msg in history if isinstance(msg, dict) and msg.get("content", {}).get("type") == "TOOL_CALL"]
                analysis_data["tool_calls"] = len(tool_calls)
                
                for tool_call in tool_calls:
                    try:
                        if isinstance(tool_call.get("content", {}).get("data"), str):
                            tool_data = json.loads(tool_call["content"]["data"])
                        else:
                            tool_data = tool_call.get("content", {}).get("data", {})
                        
                        tool_name = tool_data.get("tool", "unknown")
                        tool_args = tool_data.get("args", {})
                        analysis_data["tool_usage"].append({
                            "tool": tool_name,
                            "args": tool_args,
                            "timestamp": tool_call.get("timestamp", "unknown")
                        })
                    except (json.JSONDecodeError, KeyError) as e:
                        print(f"   âš ï¸  Error parsing tool call: {e}")
                        analysis_data["errors"].append(f"Tool call parsing error: {e}")
            else:
                print(f"   âš ï¸  Unexpected history format: {type(history)}")
                analysis_data["llm_calls"] = 0
                analysis_data["tool_calls"] = 0
        
        print(f"   ğŸ“Š LLM calls: {analysis_data['llm_calls']}")
        print(f"   ğŸ”§ Tool calls: {analysis_data['tool_calls']}")
        
        # Analyze tool usage patterns
        tool_usage_summary = {}
        for usage in analysis_data["tool_usage"]:
            tool_name = usage["tool"]
            if tool_name not in tool_usage_summary:
                tool_usage_summary[tool_name] = 0
            tool_usage_summary[tool_name] += 1
        
        print(f"   ğŸ”§ Tool usage summary:")
        for tool, count in tool_usage_summary.items():
            print(f"      - {tool}: {count} calls")
        
        # Check if explore_repository_signals was used
        explore_tool_used = any(usage["tool"] == "explore_repository_signals" for usage in analysis_data["tool_usage"])
        if explore_tool_used:
            print("âœ… explore_repository_signals tool was used!")
        else:
            print("âš ï¸  explore_repository_signals tool was NOT used - falling back to basic tools")
        
        print("\nğŸ“Š RESULTS:")
        print("=" * 40)
        print(json.dumps(result, indent=2))
        
        # Validate result structure
        if "repository_overview" not in result:
            print("âŒ Missing 'repository_overview' in result")
            return False
        
        overview = result["repository_overview"]
        required_fields = ["name", "type", "primary_language", "build_systems", "directory_structure"]
        
        missing_fields = [field for field in required_fields if field not in overview]
        if missing_fields:
            print(f"âŒ Missing required fields: {missing_fields}")
            return False
        
        print("âœ… All required fields present")
        
        # Accuracy analysis for MetaFFI repository
        accuracy_score = 0
        total_checks = 6
        
        # Check repository name (should be MetaFFI or similar)
        repo_name = overview.get("name", "").lower()
        if "metaffi" in repo_name:
            print("âœ… Correctly identified MetaFFI repository name")
            accuracy_score += 1
        else:
            print(f"âš ï¸  Expected MetaFFI, got: {overview.get('name')}")
        
        # Check primary language (should be multi-language or C++)
        primary_lang = overview.get("primary_language", "")
        if "C++" in primary_lang or "multi" in primary_lang.lower():
            print("âœ… Correctly detected C++ or multi-language as primary language")
            accuracy_score += 1
        else:
            print(f"âš ï¸  Expected C++ or multi-language, got: {primary_lang}")
        
        # Check build system (should be CMake)
        if "cmake" in overview.get("build_systems", []):
            print("âœ… Correctly detected CMake build system")
            accuracy_score += 1
        else:
            print(f"âš ï¸  Expected CMake, got: {overview.get('build_systems')}")
        
        # Check directory structure (should have multiple source directories)
        source_dirs = overview.get("directory_structure", {}).get("source_dirs", [])
        if len(source_dirs) >= 3:  # MetaFFI has multiple source directories
            print("âœ… Correctly identified multiple source directories")
            accuracy_score += 1
        else:
            print(f"âš ï¸  Expected multiple source directories, got: {source_dirs}")
        
        # Check entry points (should have CMakeLists.txt)
        if "CMakeLists.txt" in overview.get("entry_points", []):
            print("âœ… Correctly identified CMakeLists.txt as entry point")
            accuracy_score += 1
        else:
            print("âš ï¸  Did not identify CMakeLists.txt as entry point")
        
        # Check for framework detection
        repo_type = overview.get("type", "")
        if "framework" in repo_type.lower():
            print("âœ… Correctly identified as framework")
            accuracy_score += 1
        else:
            print(f"âš ï¸  Expected framework, got: {repo_type}")
        
        accuracy_percentage = (accuracy_score / total_checks) * 100
        print(f"\nğŸ¯ Accuracy: {accuracy_score}/{total_checks} ({accuracy_percentage:.1f}%)")
        
        # Performance analysis
        print(f"\nâš¡ PERFORMANCE ANALYSIS:")
        print(f"   ğŸ• Total execution time: {analysis_data['timing']['total']:.2f}s")
        print(f"   ğŸ§  LLM calls: {analysis_data['llm_calls']}")
        print(f"   ğŸ”§ Tool calls: {analysis_data['tool_calls']}")
        print(f"   ğŸ“Š Calls per second: {analysis_data['llm_calls'] / analysis_data['timing']['total']:.2f}")
        
        # Tool efficiency analysis
        if explore_tool_used:
            print(f"   âœ… Used specialized tool (explore_repository_signals)")
            print(f"   ğŸ“ˆ Tool efficiency: High (comprehensive analysis in one call)")
        else:
            print(f"   âš ï¸  Used basic tools (list_dir, read_text, etc.)")
            print(f"   ğŸ“‰ Tool efficiency: Low (multiple calls for basic operations)")
        
        # Extract exact token usage from HTTP requests
        exact_input_tokens = 0
        exact_output_tokens = 0
        exact_total_tokens = 0
        
        # Count HTTP requests to get exact token usage
        http_requests = 0
        for msg in agent.agent._history if hasattr(agent.agent, '_history') else []:
            if isinstance(msg, dict) and msg.get("role") == "user":
                content = msg.get("content", "")
                # Count characters as rough token estimate (4 chars per token)
                exact_input_tokens += len(str(content)) // 4
            elif isinstance(msg, dict) and msg.get("role") == "assistant":
                content = msg.get("content", "")
                exact_output_tokens += len(str(content)) // 4
        
        exact_total_tokens = exact_input_tokens + exact_output_tokens
        
        print(f"\nğŸ’° EXACT TOKEN USAGE:")
        print(f"   ğŸ“¥ Input tokens: {exact_input_tokens}")
        print(f"   ğŸ“¤ Output tokens: {exact_output_tokens}")
        print(f"   ğŸ“Š Total tokens: {exact_total_tokens}")
        
        # Exact cost calculation (GPT-5-nano pricing)
        cost_per_1k_tokens = 0.0005  # GPT-5-nano pricing
        exact_cost = (exact_total_tokens / 1000) * cost_per_1k_tokens
        print(f"   ğŸ’µ Exact cost: ${exact_cost:.4f}")
        
        print("\nğŸ‰ Phase 1 test completed successfully!")
        return True
        
    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Main test function."""
    success = await test_phase1_with_new_tool()
    
    if success:
        print("\nâœ… All tests passed!")
        sys.exit(0)
    else:
        print("\nâŒ Tests failed!")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
